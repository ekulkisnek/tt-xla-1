JAX USING SHARDY:

SHLO Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = <["batch"=2, "model"=4]>
  func.func public @main(%arg0: tensor<8192x784xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"batch"}, {"model"}]>}) -> (tensor<32768x784xf32> {jax.result_info = "result", sdy.sharding = #sdy.sharding<@mesh, [{"batch"}, {"model"}]>}) {
    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{"batch"}, {"model"}]>] out_shardings=[<@mesh, [{"batch"}, {"model"}]>] manual_axes={"batch", "model"} (%arg1: tensor<4096x196xf32>) {
      %1 = "stablehlo.all_gather"(%arg1) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<4096x196xf32>) -> tensor<16384x196xf32>
      sdy.return %1 : tensor<16384x196xf32>
    } : (tensor<8192x784xf32>) -> tensor<32768x784xf32>
    return %0 : tensor<32768x784xf32>
  }
}

TTIR Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh" = 2x4>]>} {
  func.func public @main(%arg0: tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> (tensor<32768x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>> {jax.result_info = "result"}) {
    %0 = ttir.empty() : tensor<4096x196xf32, #ttcore.mesh_sharding<"mesh">>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>, tensor<4096x196xf32, #ttcore.mesh_sharding<"mesh">>) -> tensor<4096x196xf32, #ttcore.mesh_sharding<"mesh">>
    %2 = ttir.empty() : tensor<16384x196xf32, #ttcore.mesh_sharding<"mesh">>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32}> : (tensor<4096x196xf32, #ttcore.mesh_sharding<"mesh">>, tensor<16384x196xf32, #ttcore.mesh_sharding<"mesh">>) -> tensor<16384x196xf32, #ttcore.mesh_sharding<"mesh">>
    %4 = ttir.empty() : tensor<32768x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<16384x196xf32, #ttcore.mesh_sharding<"mesh">>, tensor<32768x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<32768x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    return %5 : tensor<32768x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
  }
}

TTNN Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh" = 2x4>]>} {
  ttcore.device_module {
    builtin.module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh" = 2x4>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x18,  16x25,  17x21,  17x22,  17x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  17x21,  17x22,  17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x23,  16x24,  16x25,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  17x21,  17x22,  17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x23,  16x24,  16x25,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x18,  16x25,  17x21,  17x22,  17x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [3 : i32, 3 : i32, 3 : i32, 3 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<16x32, (d0, d1) -> ((d0 floordiv 8) * 4 + d1 floordiv 8, d0 mod 2, d1 mod 4)>, l1Map = (d0, d1, d2)[s0] -> ((d0 floordiv 8) * 4 + d1 floordiv 8, d0 mod 2, d1 mod 4, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 2x4, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func public @main(%arg0: tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {jax.result_info = "result"}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 2x4>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 4096 : i32, 196 : i32]}> : (tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.all_gather"(%2, %0) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, mesh = <"mesh">, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [16384 : i32, 196 : i32]}> : (tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%4, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh">, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %5 : tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, mesh = <"mesh", [ 2(0),  4(1)]>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}


JAX WITHOUT SHARDY:

SHLO Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {
  func.func public @main(%arg0: tensor<8192x784xf32> {mhlo.sharding = "{devices=[2,4]<=[8]}"}) -> (tensor<32768x784xf32> {jax.result_info = "result", mhlo.sharding = "{devices=[2,4]<=[8]}"}) {
    %0 = stablehlo.custom_call @Sharding(%arg0) {mhlo.sharding = "{devices=[2,4]<=[8]}"} : (tensor<8192x784xf32>) -> tensor<8192x784xf32>
    %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {mhlo.sharding = "{manual}"} : (tensor<8192x784xf32>) -> tensor<4096x196xf32>
    %2 = call @shmap_body(%1) : (tensor<4096x196xf32>) -> tensor<16384x196xf32>
    %3 = stablehlo.custom_call @Sharding(%2) {mhlo.sharding = "{manual}"} : (tensor<16384x196xf32>) -> tensor<16384x196xf32>
    %4 = stablehlo.custom_call @SPMDShardToFullShape(%3) {mhlo.sharding = "{devices=[2,4]<=[8]}"} : (tensor<16384x196xf32>) -> tensor<32768x784xf32>
    return %4 : tensor<32768x784xf32>
  }
  func.func private @shmap_body(%arg0: tensor<4096x196xf32>) -> (tensor<16384x196xf32> {jax.result_info = "[('batch',), ('model',)]"}) {
    %0 = "stablehlo.all_gather"(%arg0) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<4096x196xf32>) -> tensor<16384x196xf32>
    return %0 : tensor<16384x196xf32>
  }
}

TTIR Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh_gspmd" = 2x4>]>} {
  func.func public @main(%arg0: tensor<8192x784xf32>) -> (tensor<32768x784xf32> {jax.result_info = "result"}) {
    %0 = ttir.empty() : tensor<4096x196xf32>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x784xf32>, tensor<4096x196xf32>) -> tensor<4096x196xf32>
    %2 = ttir.empty() : tensor<16384x196xf32>
    %3 = "ttir.all_gather"(%1, %2) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32}> : (tensor<4096x196xf32>, tensor<16384x196xf32>) -> tensor<16384x196xf32>
    %4 = ttir.empty() : tensor<32768x784xf32>
    %5 = "ttir.mesh_shard"(%3, %4) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<16384x196xf32>, tensor<32768x784xf32>) -> tensor<32768x784xf32>
    return %5 : tensor<32768x784xf32>
  }
}

TTNN Module:
module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh_gspmd" = 2x4>]>} {
  ttcore.device_module {
    builtin.module @jit_fwd attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32, ttcore.meshes = #ttcore.meshes<[<"mesh_gspmd" = 2x4>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x18,  16x25,  17x21,  17x22,  17x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  17x21,  17x22,  17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x23,  16x24,  16x25,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  17x21,  17x22,  17x25] eth_inactive = [ 16x18,  16x19,  16x20,  16x23,  16x24,  16x25,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073174528, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x18,  16x25,  17x21,  17x22,  17x25] eth_inactive = [ 16x19,  16x20,  16x21,  16x22,  16x23,  16x24,  17x19,  17x20,  17x23,  17x24]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 97248, erisc_l1_unreserved_base = 69632, dram_unreserved_base = 32, dram_unreserved_end = 1073190752, physical_helper_cores = {dram = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  0x8,  0x9,  0x10,  0x11] eth = [ 16x21,  16x22,  16x25] eth_inactive = [ 16x19,  16x20,  16x23,  16x24,  17x18,  17x19,  17x20,  17x21,  17x22,  17x23,  17x24,  17x25]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1, 2, 3, 4, 5, 6, 7], [3 : i32, 3 : i32, 3 : i32, 3 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<16x32, (d0, d1) -> ((d0 floordiv 8) * 4 + d1 floordiv 8, d0 mod 2, d1 mod 4)>, l1Map = (d0, d1, d2)[s0] -> ((d0 floordiv 8) * 4 + d1 floordiv 8, d0 mod 2, d1 mod 4, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 2x4, chipIds = [0, 1, 2, 3, 4, 5, 6, 7]>
      func.func public @main(%arg0: tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> (tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {jax.result_info = "result"}) {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 2x4>}> : () -> !ttnn.device
        %1 = "ttnn.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<8192x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 4096 : i32, 196 : i32]}> : (tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.all_gather"(%2, %0) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32, num_links = 1 : ui32}> : (tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x4096x196xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 4096 + d1, d2), <1x1>, memref<128x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [16384 : i32, 196 : i32]}> : (tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<4x4096x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%4, %0) <{shard_dims = array<i64: 0, 1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 2, 4>, shard_type = #ttcore.shard_type<identity>}> : (tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<16384x196xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<512x7x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %5 : tensor<32768x784xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1024x25x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}

TORCH-XLA USING SHARDY:

SHLO Module:
module @SyncTensorsGraph.11 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2, "_axis_1"=4]>
  func.func @main(%arg0: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg1: tensor<8192x784xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>}) -> (tensor<16384x784xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>}) {
    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {"_axis_1"}]>]>} : (tensor<f32>) -> tensor<8192x784xf32>
    %1 = stablehlo.add %arg1, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {"_axis_1"}]>]>} : tensor<8192x784xf32>
    %2 = "stablehlo.all_gather"(%1) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, use_global_device_ids}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {"_axis_1"}]>]>} : (tensor<8192x784xf32>) -> tensor<16384x784xf32>
    return %2 : tensor<16384x784xf32>
  }
}

TTIR Module:
module @SyncTensorsGraph.11 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 2x4>]>} {
  func.func @main(%arg0: tensor<1xf32, #ttcore.mesh_sharding<"mesh">>, %arg1: tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<16384x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>> {
    %0 = ttir.empty() : tensor<1x1xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %1 = "ttir.reshape"(%arg0, %0) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32, #ttcore.mesh_sharding<"mesh">>, tensor<1x1xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<1x1xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %2 = ttir.empty() : tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %3 = "ttir.broadcast"(%1, %2) <{broadcast_dimensions = array<i64: 8192, 784>}> : (tensor<1x1xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>, tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %4 = ttir.empty() : tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %5 = "ttir.add"(%arg1, %3, %4) : (tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>, tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>, tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %6 = ttir.empty() : tensor<16384x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    %7 = "ttir.all_gather"(%5, %6) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32}> : (tensor<8192x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>, tensor<16384x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>) -> tensor<16384x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
    return %7 : tensor<16384x784xf32, #ttcore.mesh_sharding<"mesh", [ 2(0),  4(1)]>>
  }
}

TTNN Module (ERROR):
loc("all-gather.5"): error: 'ttnn.reshape' op Input tensor number of elements 25690112 and output tensor number of elements 12845056 must be the same
2025-07-02 09:38:04.996 (   3.512s) [        E8611480]      module_builder.cc:536    ERR| Failed to convert from TTIR to TTNN module
2025-07-02 09:38:04.997 (   3.513s) [        E8611480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-07-02 09:38:04.997 (   3.513s) [        E8611480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-07-02 09:38:04.997 (   3.513s) [        E8611480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
Traceback (most recent call last):
  File "/localdev/hshah/tt-xla/examples/xla_all_gather_simple.py", line 76, in <module>
    y = y.to("cpu")
RuntimeError: Error code: 13

TORCH-XLA WITHOUT SHARDY:

SHLO Module:
module @SyncTensorsGraph.11 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<f32> {mhlo.sharding = "{replicated}"}, %arg1: tensor<8192x784xf32> {mhlo.sharding = "{devices=[2,4]0,1,2,3,4,5,6,7}"}) -> tensor<16384x784xf32> {
    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<8192x784xf32>
    %1 = stablehlo.add %arg1, %0 : tensor<8192x784xf32>
    %2 = "stablehlo.all_gather"(%1) <{all_gather_dim = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<8192x784xf32>) -> tensor<16384x784xf32>
    return %2 : tensor<16384x784xf32>
  }
}

TTIR Module:
module @SyncTensorsGraph.11 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1xf32> {mhlo.sharding = "{replicated}"}, %arg1: tensor<8192x784xf32> {mhlo.sharding = "{devices=[2,4]0,1,2,3,4,5,6,7}"}) -> tensor<16384x784xf32> {
    %0 = ttir.empty() : tensor<1x1xf32>
    %1 = "ttir.reshape"(%arg0, %0) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %2 = ttir.empty() : tensor<8192x784xf32>
    %3 = "ttir.broadcast"(%1, %2) <{broadcast_dimensions = array<i64: 8192, 784>}> : (tensor<1x1xf32>, tensor<8192x784xf32>) -> tensor<8192x784xf32>
    %4 = ttir.empty() : tensor<8192x784xf32>
    %5 = "ttir.add"(%arg1, %3, %4) : (tensor<8192x784xf32>, tensor<8192x784xf32>, tensor<8192x784xf32>) -> tensor<8192x784xf32>
    %6 = ttir.empty() : tensor<16384x784xf32>
    %7 = "ttir.all_gather"(%5, %6) <{all_gather_dim = 0 : si32, cluster_axis = 1 : ui32}> : (tensor<8192x784xf32>, tensor<16384x784xf32>) -> tensor<16384x784xf32>
    return %7 : tensor<16384x784xf32>
  }
}

TTNN Module:
loc("all-gather.5"): error: 'ttnn.reshape' op Input tensor number of elements 25690112 and output tensor number of elements 12845056 must be the same
2025-07-02 10:51:11.966 (   3.530s) [        17CE5480]      module_builder.cc:536    ERR| Failed to convert from TTIR to TTNN module
2025-07-02 10:51:11.967 (   3.531s) [        17CE5480]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-07-02 10:51:11.967 (   3.531s) [        17CE5480]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-07-02 10:51:11.967 (   3.531s) [        17CE5480]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
Traceback (most recent call last):
  File "/localdev/hshah/tt-xla/examples/xla_all_gather_simple.py", line 76, in <module>
    y = y.to("cpu")
RuntimeError: Bad StatusOr access: INTERNAL: Error code: 13