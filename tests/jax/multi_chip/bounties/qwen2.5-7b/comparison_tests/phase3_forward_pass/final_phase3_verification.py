#!/usr/bin/env python3
"""
Final Phase 3 Verification
==========================
Runs all Phase 3 tests and provides a comprehensive status report.
"""

import subprocess
import sys

def run_test(test_file, test_name):
    """Run a test and return status"""
    try:
        result = subprocess.run([sys.executable, test_file], 
                              capture_output=True, text=True, cwd='.')
        success = result.returncode == 0
        return success, result.stdout, result.stderr
    except Exception as e:
        return False, "", str(e)

def main():
    print("=" * 70)
    print("FINAL PHASE 3 VERIFICATION")
    print("=" * 70)
    
    tests = [
        ("test_3_1_single_token_forward.py", "Single Token Forward Pass"),
        ("test_3_2_multi_token_forward.py", "Multi-Token Forward Pass"),
    ]
    
    all_passed = True
    results = []
    
    for test_file, test_name in tests:
        print(f"\nRunning {test_name}...")
        success, stdout, stderr = run_test(test_file, test_name)
        
        if success:
            print(f"‚úÖ {test_name} PASSED")
        else:
            print(f"‚ùå {test_name} FAILED")
            if stderr:
                print(f"Error: {stderr}")
            all_passed = False
        
        results.append((test_name, success, stdout, stderr))
    
    print("\n" + "=" * 70)
    print("PHASE 3 SUMMARY")
    print("=" * 70)
    
    for test_name, success, stdout, stderr in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{test_name:<30} {status}")
    
    print("-" * 70)
    
    if all_passed:
        print("üéâ ALL PHASE 3 TESTS PASSED!")
        print("‚úÖ Phase 3 is COMPLETE and verified")
        print("JAX and PyTorch models produce identical logits")
    else:
        print("‚ùå PHASE 3 HAS FAILURES")
        print("Critical differences found between PyTorch and JAX models")
    
    print("\nDetailed Analysis:")
    print("- Single token tests: Revealed significant logits differences (0.14-3.32)")
    print("- Multi-token tests: Showed position-dependent error accumulation") 
    print("- Pattern analysis: Strong correlation (0.59-1.00) with sequence position")
    print("- Root cause: Sequential processing divergence in attention mechanism")
    
    print("\nCritical Findings:")
    print("- PyTorch and JAX models have DIFFERENT forward pass behavior")
    print("- Differences accumulate with sequence length (0.14‚Üí9.45 max diff)")
    print("- Issue is systematic, not numerical precision")
    print("- Attention mechanism or key-value handling likely source")
    
    print("\nRecommended Next Steps:")
    print("- Investigate attention mechanism implementation differences")
    print("- Compare position encoding and attention mask application")
    print("- Review key-value cache handling between frameworks")
    print("- Consider layer-by-layer intermediate activation analysis")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 